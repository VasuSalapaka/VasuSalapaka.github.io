<!doctype html>
<html>
<head>
	
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link href="style.css" rel="stylesheet" type="text/css">
<link href="css/bootstrap-4.4.1.css" rel="stylesheet" type="text/css">
<script  src="includehtmlx.js"></script>
</head>	
	
<body>
<div id="accordion1" role="tablist">
	  <div class="card">
	    <div class="card-header" role="tab" id="headingOne1">
	      <h5 class="mb-0"> <a data-toggle="collapse" href="#collapseOne1" role="button" aria-expanded="true" aria-controls="collapseOne1"> <li class="lisquare">  
<span class="l2head">  Maximum Entropy Principle based Framework for Paramerterized Markov Decision Processes and Reinforcement learning</span> 
			 <!-- <a  href="pdfs/baranwal2016vehicle.pdf">    <img src="images/pdf.jpg" alt=" " width:auto height="20" border="0"> </a> 
<a   href="videos/AllocationRouting.mp4">    <img src="Images/video.jpg" alt=" " width:auto height="20" border="0"> </a> -->
 			  
		  </li></a> </h5>
        </div>
	    <div id="collapseOne1" class="collapse" role="tabpanel" aria-labelledby="headingOne1" data-parent="#accordion1">
	      <div class="card-body">We developed a framework to address a class of sequential decision making problems. Our framework features learning the optimal control policy with robustness to noisy data, determining the unknown state and action parameters, and performing sensitivity analysis with respect to problem parameters. We consider two broad categories of sequential decision making problems modelled as infinite horizon Markov Decision Processes (MDPs) with (and without) an absorbing state. The central idea underlying our framework is to quantify exploration in terms of the Shannon Entropy of the trajectories under the MDP and determine the stochastic policy that maximizes it while guaranteeing a low value of the expected cost along a trajectory. This resulting policy enhances the quality of exploration early on in the learning process, and consequently allows faster convergence rates and robust solutions even in the presence of noisy data as demonstrated in our comparisons to popular algorithms such as Q-learning, Double Q-learning and entropy regularized Soft Q-learning. The framework extends to the class of parameterized MDP and RL problems, where states and actions are parameter dependent, and the objective is to determine the optimal parameters along with the corresponding optimal policy. Here, the associated cost function can possibly be non-convex with multiple poor local minima. Simulation results applied to a 5G small cell network problem demonstrate successful determination of communication routes and the small cell locations. We also obtain sensitivity measures to problem parameters and robustness to noisy environment data.</div>
        </div>
  </div>
		  
		  
</div>



<div id="accordion2" role="tablist">
	  <div class="card">
	    <div class="card-header" role="tab" id="headingtwo2">
	      <h5 class="mb-0"> <a data-toggle="collapse" href="#collapsetwo2" role="button" aria-expanded="true" aria-controls="collapsetwo2"> <li class="lisquare">  
<span class="l2head">  Quantifying Persistence of an Aggregated Markov Chain and Estimating Its Natural Number of Super-states</span> 
			 <!-- <a  href="pdfs/baranwal2016vehicle.pdf">    <img src="images/pdf.jpg" alt=" " width:auto height="20" border="0"> </a> 
<a   href="videos/AllocationRouting.mp4">    <img src="Images/video.jpg" alt=" " width:auto height="20" border="0"> </a> -->
 			  
		  </li></a> </h5>
        </div>
	    <div id="collapsetwo2" class="collapse" role="tabpanel" aria-labelledby="headingtwo2" data-parent="#accordion2">
	      <div class="card-body">Many studies involving large Markov chains require determining a smaller representative (aggregated) chain. Each {\em superstate} in the representative chain represents a {\em group of related} states in the original Markov chain. Typically, the choice of number of superstates in the aggregated chain is ambiguous, and based on the limited prior know-how. In this paper we present a structured methodology of determining the best candidate for the number of superstates. We achieve this by comparing aggregated chains of different sizes. To facilitate this comparison we develop and quantify a notion of {\em marginal return}. Our notion captures the decrease in the {\em heterogeneity} within the group of {\em related} states (i.e., states represented by the same superstate) upon a unit increase in the number of superstates in the aggregated chain. We use Maximum Entropy Principle to justify the notion of marginal return, as well as our quantification of heterogeneity. Through simulations on synthetic Markov chains, where the number of superstates are known apriori, we show that the aggregated chain with the largest marginal return identifies this number. In case of Markov chains that model real-life scenarios we show that the aggregated model with the largest marginal return identifies an inherent structure unique to the scenario being modelled; thus, substantiating on the efficacy of our proposed methodology.</div>
        </div>
  </div>
	
	  
		  
		  
		  
</div>

<script src="js/jquery-3.4.1.min.js"></script>
<script src="js/popper.min.js"></script>
<script src="js/bootstrap-4.4.1.js"></script>
</body>
</html>